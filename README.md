# Données utilisées
Malheureusement mon ordinateur n'a pas pu faire tourner toutes les données disponibles
 à l'adresse indiquée.
J'ai choisi d'utiliser les données de 2012 à 2014 en guise d'entrainement 
et celle de 2015 pour le test.

# Modèle choisi
J'ai choisi d'utilise des modèles de machine learning de scikit learn, des modèles de 
deep learning ne serait pas intéressant pour le problème proposé car il est suffisamment
 simple pour être traité avec du machine learning classique.

# Explication notebook
J'ai essayé de laissé un maximum de commentaire afin que vous puissiez suivre mon raisonnement.
J'ai evidemment laissé le résultat des cellules de mon notebook, mais je vous envoie quand même les données que j'ai utilisé pour pouvoir relancer le notebook si nécessaire.

# Réponse aux questions posées dans le sujet

### If you are struggling to implement something that deals with this volume of data, do you know of a way to deal with it in theory ?

Au vu de la taille des données, j'imagine qu'un ordinateur un peut plus puissant devrait être capable de faire tourner ce genre de modèle, l'ordinateur que j'utilise actuellement est un peu vieux.
Pour ma part, la RAM ainsi que la capacité de mon processeur est le problème, j'imagine qu'il est possible d'effectuer ces calculs dans un cloud pour pallier à ce problème.

### If you are having trouble extracting features, can you submit an evaluation of a sensible baseline on the test data ?

Certaine feature mériterait d'être ajouté dans le modèle que j'ai proposé comme l'ai indiqué dans mon notebook.

Aussi certaines feature pourrait utiliser un encodage différent, comme un one encoding. Il faudrait passer un peu plus de temps sur le sujet pour tester ces questions. 
